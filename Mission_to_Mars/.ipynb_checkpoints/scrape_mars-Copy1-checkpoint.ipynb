{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function the visits a webpage and returns a beautful soup\n",
    "def get_soup(base_url, args):\n",
    "    \n",
    "    # Base url plus any search terms\n",
    "    url = base_url + args\n",
    "    \n",
    "    #executable_path = {'chromedriver.exe': '/usr/local/bin/chromedriver'}\n",
    "    browser = Browser('chrome', headless=False)\n",
    "    \n",
    "    # Visit with selenium\n",
    "    browser.visit(url)\n",
    "\n",
    "    # Wait 1 second to allow the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Save the browser's html as text\n",
    "    html = browser.html\n",
    "\n",
    "    # Convert html text to beautiful soup object\n",
    "    soup = bs(html, 'html.parser')\n",
    "\n",
    "    # Print the webpage title\n",
    "    print(soup.title.text +'\\n')\n",
    "    \n",
    "    return soup\n",
    "\n",
    "# Function that scrapes News Title and Summary from the Nasa Mars News site and returns a list of dictionaries\n",
    "def get_news(base_url, args):\n",
    "    \n",
    "    url = base_url + args\n",
    "    \n",
    "    # Get soup\n",
    "    soup = get_soup(url)\n",
    "\n",
    "    # Identify the Title and Summary via div.content_title and div.article_teaser_body\n",
    "    results = soup.find_all('div', class_=\"list_text\")\n",
    "\n",
    "    # news is a list of dictionaries\n",
    "    news = []\n",
    "    # Iterate over the results \n",
    "    for result in results:\n",
    "\n",
    "        # Only Retrieve results if they have a summary text\n",
    "        try:\n",
    "            date = result.find('div', class_=\"list_date\").text\n",
    "            summary = result.find('div', class_=\"article_teaser_body\").text\n",
    "            headline = result.find('div', class_=\"content_title\").get_text()\n",
    "            #print(f\"{date} : {headline}\\n{summary}\\n\\n\")\n",
    "\n",
    "            # Save results to a dictionary\n",
    "            dictionary = {}\n",
    "            dictionary= {\n",
    "                'date' : date,\n",
    "                'headline' : headline,\n",
    "                'summary' : summary\n",
    "            }\n",
    "\n",
    "            # Append list with dictionary\n",
    "            news.append(dictionary)\n",
    "\n",
    "        # If the article is missing content, skip it\n",
    "        except:\n",
    "            print(\"Nothing found\")\n",
    "\n",
    "    return news\n",
    "\n",
    "# Function which scrapes the JPL 'Featured Image' and returns the url to the image\n",
    "def get_featured_img(base_url, args):\n",
    "    \n",
    "    url = base_url + args\n",
    "    \n",
    "    soup = get_soup(url)\n",
    "\n",
    "    # Identify the featured image url via section.centered_text clearfix main_feature primary_media_feature single\n",
    "    result = soup.find_all('section', class_=\"centered_text clearfix main_feature primary_media_feature single\")[0]\n",
    "\n",
    "    # Split the string with '(' or ')'\n",
    "    string = re.split('[(|)]', result.article['style'])[1]\n",
    "\n",
    "    # Strip the single quotes\n",
    "    string = string[1:-1]\n",
    "\n",
    "    # Combine with base url\n",
    "    image_url = base_url + string\n",
    "\n",
    "    return image_url\n",
    "\n",
    "# A function which scrapes Mars weather data from the Mars Weather Twitter Page and returns the latest weather\n",
    "def get_weather(base_url, args):\n",
    "    \n",
    "    url = base_url + args\n",
    "    \n",
    "    soup = get_soup(url)\n",
    "\n",
    "    # Identify the weather report via tweet -> tweet-text\n",
    "    results = soup.find_all('div', class_=\"tweet\")\n",
    "\n",
    "    # The latest tweet is in the first result; save in a variable\n",
    "    mars_weather = results[0].find('p', class_=\"tweet-text\").text\n",
    "\n",
    "    # Remove the link off the end\n",
    "    mars_weather = mars_weather.split('pic.twitter')[0]\n",
    "\n",
    "    # The first result contains the latest tweet.\n",
    "    print('Latest weather report:''\\n\\n' + mars_weather + '\\n\\n')\n",
    "\n",
    "    ## Print all tweets for good measure\n",
    "    #for result in results:\n",
    "    #    print(result.find('p', class_=\"tweet-text\").get_text())\n",
    "    \n",
    "    return mars_weather\n",
    "\n",
    "# Function which scapes Mars facts from space-facts.com/mars and returns a dataframe\n",
    "def get_facts(base_url, args):\n",
    "    \n",
    "    url = base_url + args\n",
    "    \n",
    "    # Scrape for data tables with pandas\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    # The relevant table is the first table\n",
    "    return tables[0]\n",
    "\n",
    "# Function which scrapes images of the Mars Hemispheres and returns a list of dicntionaries of titles and urls\n",
    "def get_hemis(base_url, args):\n",
    "    \n",
    "    url = base_url + args\n",
    "\n",
    "    # Get soup of the main page\n",
    "    soup = get_soup(url)\n",
    "\n",
    "    # Find urls to high-res images via section -> results-accordian -> div.item, a.href.text \n",
    "    results = soup.find_all('section', {'id':'results-accordian'})[0].find_all('div', class_='item')\n",
    "    inter_urls = []\n",
    "\n",
    "    # Iterate over each link to the sub-page\n",
    "    for result in results:\n",
    "        try:\n",
    "            # Find urls via ->a.hef\n",
    "            inter_urls.append(base_url+result.a['href'])\n",
    "        except:\n",
    "            print(\"null\")\n",
    "\n",
    "    # \n",
    "    hemi_list = []\n",
    "    for url in inter_urls:\n",
    "        # Visit each link\n",
    "        soup = get_soup(url)\n",
    "        try:\n",
    "            img_url = soup.find_all('div', class_='downloads')[0].find_all('li')[0].a['href']\n",
    "            img_title = soup.title.text\n",
    "            img_title = img_title.split(\" |\")[0]\n",
    "            dictionary = {\n",
    "                'title' : img_title,\n",
    "                'img_url' : img_url\n",
    "            }\n",
    "            hemi_list.append(dictionary)\n",
    "        except:\n",
    "            print(\"null\")\n",
    "\n",
    "    return hemi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News  – NASA’s Mars Exploration Program \n",
      "\n",
      "Space Images\n",
      "\n",
      "Mars Weather (@MarsWxReport) | Twitter\n",
      "\n",
      "Latest weather report:\n",
      "\n",
      "InSight sol 310 (2019-10-10) low -102.2ºC (-152.0ºF) high -26.6ºC (-15.8ºF)\n",
      "winds from the SSE at 5.0 m/s (11.2 mph) gusting to 19.1 m/s (42.8 mph)\n",
      "pressure at 7.20 hPa\n",
      "\n",
      "\n",
      "Astropedia Search Results | USGS Astrogeology Science Center\n",
      "\n",
      "Cerberus Hemisphere Enhanced | USGS Astrogeology Science Center\n",
      "\n",
      "Schiaparelli Hemisphere Enhanced | USGS Astrogeology Science Center\n",
      "\n",
      "Syrtis Major Hemisphere Enhanced | USGS Astrogeology Science Center\n",
      "\n",
      "Valles Marineris Hemisphere Enhanced | USGS Astrogeology Science Center\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get news about Mars\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "args = ''\n",
    "news = get_news(url,'')\n",
    "#print(news)\n",
    "\n",
    "# Get featured image\n",
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "args = '/spaceimages/?search=&category=Mars'\n",
    "image_url = get_featured_img(base_url, args)\n",
    "\n",
    "# Get weather information\n",
    "url = 'https://twitter.com/marswxreport?lang=en'\n",
    "args = ''\n",
    "mars_weather = get_weather(url, args)\n",
    "\n",
    "# Get Mars facts\n",
    "url = 'https://space-facts.com/mars'\n",
    "args = ''\n",
    "fact_table = get_facts(url,args)\n",
    "\n",
    "# Get images of hemispheres\n",
    "base_url = 'https://astrogeology.usgs.gov/'\n",
    "args = 'search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "hemi_list = get_hemis(base_url, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
